---
title: "Final Assignment"
author: "Holly Finertie"
date: "Due: 5/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggbiplot)
library(stats)
library(factoextra)
library(cluster)
library(caret)
library(randomForest)
library(e1071)
set.seed(100)
```


## Part 1: Neighborhood Environments in NYC

### Question 1: Run Unsupervised Learning Analysis

#### Load Data
```{r}
nyc_data = read_csv("./nyc_env_correct.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::rename(old_housing = pct_pre1960_housing, 
                npl_site = npl_site_count_wt_by_dist)

```

#### Scale Data

```{r}
# mean and sd are different
summary(nyc_data)
apply(nyc_data, 2, sd, na.rm = TRUE)

# scaling
nyc_data = scale(nyc_data)
```

#### K-Means Clustering

```{r}
# Visually test out 5 clusters
clusters = kmeans(nyc_data, 5, nstart=25)
str(clusters)
fviz_cluster(clusters, data=nyc_data)

# Find optimal using gap statistic
gap_stat = clusGap(nyc_data, FUN=kmeans, nstart=25, K.max=9, B=50)
print(gap_stat, method="firstmax")

# Visualize optimal clusters
clusters_new = kmeans(nyc_data, 2, nstart = 25)
str(clusters_new)
fviz_cluster(clusters_new, data=nyc_data)
```


### Question 2: Describe Outputs

First, I imported 2,140 rows of data on diesel exhaust, prosimity to NPL sites, proportion of housing built before 1960, and proprotion of the neighborhood with household income lower than the city median for each NYC census tract. Because the means and standard deviations varied, I scaled the data to make it compatible for clustering analysis. 

Once the data was imported and cleaned, I decided to run K-Means clustering analysis. I chose this method instead of hierarchical clustering because our data was large and, visually, the K-Means plot would be more useful than a dendogram. 

Initially, we visually assessed 5 clusters across the 2,140 inputs with their 4 corresponding variables. Then, we ran a gap statistic analysis to determine the optimal number of clusters. This analysis determined that 2 clusters best fit the data. While there are a few notalbe outliers (observation 392 and 394), we can visually confirm that 2 clusters fit the data appropriately in our Cluster Plot. 

Overall, this analysis shows us that there are 2 main clusters of census tracts based on our 4 variables of interests. 

### Question 3: Research Question

I would include the cluster assignement as a confounding variable in an explanatory analysis to see if race explains adverse health outcomes. 

**Questions**: 

* Preliminary Analysis: Is an individuals race associated with adverse health events (yes/no) in NYC? 

* Secondary Analysis: Is an individuals race associated with adverse health events in NYC if adjusted by a confounding variable created with information on diesel exhaust, proximity to NPL sites, housing before 1960, and low household income by census tract? 


## Part 2: Supervised Adventure (B)

### Choice B

#### Load Data

```{r}
glucose_data = read_csv("./glucose.csv") %>% 
  janitor::clean_names() %>% 
  drop_na() %>%
  mutate(
    riagendr = as.factor(riagendr), 
    ridreth1 = as.factor(ridreth1)
  ) %>% 
  select(-seqn)
```

#### Testing and Training Data Sets
```{r}
train = glucose_data %>% sample_frac(.7)
test = anti_join(glucose_data, train, by = 'x1') %>% 
  select(-x1)

train = train %>% 
  select(-x1)
```


#### Algorithm 1: LASSO
```{r}
lambda = 10^seq(-3,3, length = 100)

LASSO = train(
    lbxsgl ~., 
    data = train,
    method = "glmnet",
    trControl = trainControl("cv", number = 10), 
    tuneGrid = expand.grid(alpha = 1, lambda = lambda))

LASSO$bestTune

coef(LASSO$finalModel, LASSO$bestTune$lambda)

```


#### Algorithm 2: Random Forrest
```{r}
# Try mtry of all, half of all, sqrt of all, 
# Try ntree of 100, 300, 500
mtry_options = c((ncol(train)-1), 
               (ncol(train)-1)/2, 
               sqrt(ncol(train)-1))

mtry_grid = expand.grid(mtry = mtry_options)

tree_options = seq(100, 1000, by = 100)

rf_results = list()
for (ntree in tree_options){
    rf_glucose = train(lbxsgl ~ ., 
                      data = train,
                      method = "rf", 
                      tuneGrid = mtry_grid, 
                      importance = TRUE, 
                      
                      ntree = ntree)
    
    index = toString(ntree)
  rf_results[[index]] = rf_glucose$results
}

rf_output = bind_rows(rf_results, .id = "ntrees")

best.tune = rf_output[which.max(rf_output[,"Accuracy"]),]

rf_final = randomForest(Diabetes ~., 
                        data = train, 
                        mtry = best.tune$mtry, 
                        importance = TRUE, 
                        ntree = as.numeric(best.tune$ntrees))

rf.nhanes.final
varImp(rf.nhanes.final)
varImpPlot(rf.nhanes.final)
```


## Part 3: Ethical Considerations

### **Question 1**:   

Underrepresentation of certain populations is a huge potential risk. If researchers use social media data to develope intervetions, then they might only be targeting those who are frequent users of social media. Using the most extreme example of Twitter, we can see that using social media data will not be generalizable to the many populations. 

A Pew Research Center [publication](https://www.pewresearch.org/internet/2019/04/24/sizing-up-twitter-users/) in 2019 described the demographics of Twitter. In general, Twitter users are younger, more educated, and wealthier. Those are just the users. If you consider that 10% of users generate 90% of the content, then the differences are more extreme. 

If we use algorithms based on social media data to identify ways to increase early detection and prevention stratgies, these interventions will work best on younger, educated, and weathly populations. This will further increase disparities and health inequities. 


### **Question 2**:   

Solving underrepresentation and bias in data is extremely difficult. If researchers had access to demographic data for each social media user, they could possibly create a data set that is representative of the population and attempt to balance the data. However, this is most likely not feasible as most social media users do not report their demographic data. 

Instead, researchers should use multiple data sources (Facebook, Twitter, Tumblr, etc) because each site has different user populations. By creating a diversified data set, there will be less underrepresentation. However, this solution comes with its own issues. Trying to uniformly create one data set from multiple social media sources could be next to impossible due to their differing nature. 

Overall, it is almost impossible to solve underrepresentation in data; however, researchers can diversify their data sources and know that there will be limitations. 






